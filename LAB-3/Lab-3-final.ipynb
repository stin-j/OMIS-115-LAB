{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b3b1e0",
   "metadata": {},
   "source": [
    "# General Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7ebe",
   "metadata": {},
   "source": [
    "# Additional Import Code for dataset BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "x, y = load_breast_cancer(return_X_y=True)\n",
    "print(x, type(x),\"\\n\")\n",
    "print(y, type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45640d0b",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## For the 'mean radius' and 'mean perimeter' features, plot histograms separated by the target class (i.e., create one histogram of 'mean radius' for benign tumors and another for malignant tumors). The GaussianNB classifier assumes that continuous features follow a Gaussian (normal) distribution. Based on your plots, does the assumption of a normal distribution appear reasonable for these features within each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 1 BEGIN ############\")\n",
    "data = load_breast_cancer()\n",
    "df_bc = pd.DataFrame(x, columns=data.feature_names)\n",
    "df_bc['target'] = y\n",
    "\n",
    "# plot histograms for mean radius and mean perimeter (target class)\n",
    "features = ['mean radius', 'mean perimeter']\n",
    "target_names = ['malignant', 'benign']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# mean radius\n",
    "df_bc[df_bc['target'] == 0][features[0]].plot(\n",
    "    kind='hist', alpha=0.6, ax=axes[0], label=target_names[0], bins=30\n",
    ")\n",
    "df_bc[df_bc['target'] == 1][features[0]].plot(\n",
    "    kind='hist', alpha=0.6, ax=axes[0], label=target_names[1], bins=30\n",
    ")\n",
    "axes[0].set_title(f\"histogram of mean radius by class\")\n",
    "axes[0].set_xlabel(features[0])\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "# mean perimeter\n",
    "df_bc[df_bc['target'] == 0][features[1]].plot(\n",
    "    kind='hist', alpha=0.6, ax=axes[1], label=target_names[0], bins=30\n",
    ")\n",
    "df_bc[df_bc['target'] == 1][features[1]].plot(\n",
    "    kind='hist', alpha=0.6, ax=axes[1], label=target_names[1], bins=30\n",
    ")\n",
    "axes[1].set_title(f\"histogram of mean perimeter by class\")\n",
    "axes[1].set_xlabel(features[1])\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBased on the histograms, both features for both classes appear roughly normal. The assumption is reasonable.\")\n",
    "print(\"############# Assignment 3 Question 1 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f88cc",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## The naive assumption assumes that features are independent of each other. Generate a correlation matrix for the numerical features in the dataset. Do you see any highly correlated features? Explain how the model can still perform reasonably well even if this core assumption is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 2 BEGIN ############\")\n",
    "\n",
    "# generate a correlation matrix\n",
    "correlation_matrix = df_bc.corr()\n",
    "print(correlation_matrix)\n",
    "print()\n",
    "\n",
    "# identify highly correlated features\n",
    "print(\"Here are the highly correlated features from our correlation matrix:\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"mean area and worst area: 0.959213\")\n",
    "print(\"mean area and worst perimeter: 0.959120\")\n",
    "print(\"mean area and worst radius: 0.962746\")\n",
    "print(\"mean concave points and worst concave points: 0.910155\")\n",
    "print(\"mean concavity and mean concave points: 0.921391\")\n",
    "print(\"mean perimeter and mean area: 0.986507\")\n",
    "print(\"mean perimeter and worst area: 0.941550\")\n",
    "print(\"mean perimeter and worst perimeter: 0.970387\")\n",
    "print(\"mean perimeter and worst radius: 0.969476\")\n",
    "print(\"mean radius and mean area: 0.987357\")\n",
    "print(\"mean radius and mean perimeter: 0.997855\")\n",
    "print(\"mean radius and worst area: 0.941082\")\n",
    "print(\"mean radius and worst perimeter: 0.965137\")\n",
    "print(\"mean radius and worst radius: 0.969539\")\n",
    "print(\"mean texture and worst texture: 0.912045\")\n",
    "print(\"perimeter error and area error: 0.937655\")\n",
    "print(\"radius error and area error: 0.951830\")\n",
    "print(\"radius error and perimeter error: 0.972794\")\n",
    "print(\"worst perimeter and worst area: 0.977578\")\n",
    "print(\"worst radius and worst area: 0.984015\")\n",
    "print(\"worst radius and worst perimeter: 0.993708\")\n",
    "print(\"All of these had a strong correlation of over 0.9\")\n",
    "print()\n",
    "\n",
    "# explain how model performs well even if features aren't independent\n",
    "print(\"The model performs well even if the features aren't independent because Naive Bayes is only predicting which classification is more probable or more likely, rather than trying to calculate the true underlying probabilities. We just use independence to simplify the calculation.\")\n",
    "\n",
    "print(\"############# Assignment 3 Question 2 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a300d75",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Train GaussianNB classifier and report its accuracy on the training and test set. Remember general test/train split and random_state instructions from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 3 BEGIN ############\")\n",
    "\n",
    "# creating train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, train_size=0.75, random_state=0)\n",
    "\n",
    "# creating naive bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "# evaluating the model\n",
    "y_pred = gnb.predict(x_test)\n",
    "print(f\"Accuracy (train set): {gnb.score(x_train, y_train):.4f}\")\n",
    "print(f\"Accuracy (test set): {gnb.score(x_test, y_test):.4f}\")\n",
    "print(\"Number of mislabeled test data points out of a total %d points : %d\"\n",
    "      % (x_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "print(\"############# Assignment 3 Question 3 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29b9ca",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "## Plot the confusion matrix for this classifier, and also compute “recall” for the malignant class. In the context of cancer diagnosis, why is this metric (also known as sensitivity) often considered more critical than precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab47a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 4 BEGIN ############\")\n",
    "\n",
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# compute recall for the malignant class (class 0)\n",
    "recall_malignant = recall_score(y_test, y_pred, pos_label=0)\n",
    "print(f\"\\nRecall of malignant class: {recall_malignant:.4f}\")\n",
    "\n",
    "# explain why sensitivity > precision for cancer diagnosis\n",
    "print(\"Sensitivity/recall is more critical than precision in cancer diagnosis because missing a malignant case (false negative) can have severe consequences for the patient. It is more important to correctly identify all malignant cases, even if it means some benign cases are incorrectly flagged which are false positives.\")\n",
    "\n",
    "print(\"############# Assignment 3 Question 4 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9b044",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "## The CategoricalNB model cannot handle continuous data. To use it, we must first discretize (or \"bin\") our features. Using KBinsDiscretizer with n_bins=4 and encode='ordinal', transform the entire feature set X into a discretized version. Create a new training and test split using this fully discretized dataset and train a CategoricalNB model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40647eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 5 BEGIN ############\")\n",
    "\n",
    "# transform continuous data into discrete categories\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=4, encode='ordinal')\n",
    "x_discrete = encoder.fit(x).transform(x)\n",
    "x_train_discrete, x_test_discrete, y_train_discrete, y_test_discrete = train_test_split(x_discrete, y, test_size=0.25, train_size=0.75, random_state=0)\n",
    "\n",
    "# train categoricalnb model with transformed data\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "cnb = CategoricalNB()\n",
    "cnb.fit(x_train_discrete, y_train_discrete)\n",
    "y_pred_discrete = cnb.predict(x_test_discrete)\n",
    "print(\"CategoricalNB model predictions w/ discretized features:\\n\", str(y_pred_discrete))\n",
    "\n",
    "print(\"############# Assignment 3 Question 5 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d83f9",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "## Report its accuracy on the discretized test set. How does its performance compare to the GaussianNB model? Briefly explain why their performances might differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7dd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 6 BEGIN ############\")\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Accuracy on the discretized test set:\")\n",
    "print(f\"Accuracy (train set): {cnb.score(x_train_discrete, y_train_discrete):.4f}\")\n",
    "print(f\"Accuracy (test set): {cnb.score(x_test_discrete, y_test_discrete):.4f}\")\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (x_test_discrete.shape[0], (y_test_discrete != y_pred_discrete).sum()))\n",
    "\n",
    "# compare and explain different in performance\n",
    "print(\"\\nThe test accuracy for CategoricalNB was 0.9301 while the test accuracy of GaussianNB was 0.9371. GaussianNB performed better.\")\n",
    "print(\"This performance difference is due to the information lost from our dataset after binning; features went from having infinite distinctions (continuous numerical data) to being ordinal categories.\")\n",
    "\n",
    "print(\"############# Assignment 3 Question 6 END #############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e51f92",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "## The CategoricalNB classifier has a hyperparameter called alpha which controls Laplace smoothing. What problem does this smoothing solve in the context of our discretized data? What could happen if a specific bin for a feature (e.g., the highest bin for 'mean radius') was present in the test set but never appeared in the training set for malignant cases, and we were not using any smoothing (alpha=0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225f772-ca0c-4750-8db0-74b3c87a4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 3 Question 7 BEGIN ############\")\n",
    "\n",
    "# what does smoothing mean in this context\n",
    "print(\"The problem that smoothing solves in our discretized data is if a certain bin of a feature doesn't appear in our training data, then the model will predict a 'No' or 'Benign' tumor when it encounters that bin in testing data.\")\n",
    "print(\"This happens even if other features predict high probability of a 'malignant' tumor since the probabilities are multiplied (zeroed-out).\\n\")\n",
    "\n",
    "# what happens if a bin was present for test set but not training sets but we didn't use smoothing\n",
    "print(\"If there was a feature where a bin appeared in our test set but not in the training set for malignant cases, then the probability of YES (i.e. malignant diagnosis) given that feature (e.g. mean radius) will be 0 (false) no matter what.\")\n",
    "print(\"Without smoothing, that feature would zero-out the other features' probabilities of malignant tumor, leading to a false negative (we diagnose as benign when really it is malignant).\")\n",
    "\n",
    "print(\"############# Assignment 3 Question 7 END #############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f6880-10a0-406a-ad79-b0427f673b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
