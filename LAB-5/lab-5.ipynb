{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff54f882",
   "metadata": {},
   "source": [
    "# General Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a884d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3927a",
   "metadata": {},
   "source": [
    "# Additional Import Code for dataset C (PART 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "california_housing.frame.head()\n",
    "X = california_housing.data \n",
    "y = california_housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa1321",
   "metadata": {},
   "source": [
    "# Additional Import Code for dataset W (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "X_1 = wine_quality.data.features \n",
    "y_1 = wine_quality.data.targets['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b45d4",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313eed1",
   "metadata": {},
   "source": [
    "## Question 1: Baseline Model: Train a RandomForestRegressor with n_estimators=100. What is the R-squared (R2) score on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f52c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 1 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 1 Question 1 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db9446",
   "metadata": {},
   "source": [
    "## Question 2: Number of Trees: The n_estimators parameter defines the number of trees in the forest. Train two additional models: one with n_estimators=5 and another with n_estimators=500. How does the R2 score change as the number of trees increases? Why does a random forest generally not overfit by simply adding more trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 2 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 1 Question 2 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f123e2c",
   "metadata": {},
   "source": [
    "## Question 3: Feature Sub-sampling: The power of random forests comes from decorrelating the trees. The max_features parameter controls this. Train a model with max_features=0.5 (using 50% of features for each tree) and compare its performance to a model with max_features=None (which is equivalent to a Bagging Regressor). Which performs better, and why does this feature sub-sampling often lead to a more robust model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 3 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 1 Question 3 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cc48c",
   "metadata": {},
   "source": [
    "## Question 4: Comparison to a Single Tree: Train a single DecisionTreeRegressor with no depth constraints on the same data. How does its R2 score compare to your best RandomForestRegressor? Explain conceptually why an ensemble of trees (Random Forest) typically outperforms a single, complex tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 4 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 1 Question 4 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c86e31",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b97fb",
   "metadata": {},
   "source": [
    "## Question 1: Boosting: Train a GradientBoostingClassifier with n_estimators=100 on the combined wine dataset. Report its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 1 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 1 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582863c1",
   "metadata": {},
   "source": [
    "## Question 2: Boosting vs. Bagging: How does the accuracy of the GradientBoostingClassifier compare to a RandomForestClassifier (a bagging method) with the same n_estimators? Explain the fundamental difference in how boosting and bagging build their sequential vs. parallel ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37005771",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 2 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 2 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2765ee",
   "metadata": {},
   "source": [
    "## Question 3: Hard Voting Ensemble (Stacking): Create a VotingClassifier that combines three different base models: a LogisticRegression(max_iter=2000), a DecisionTreeClassifier(max_depth=5), and a KNeighborsClassifier(n_neighbors=7). Use the default voting='hard'.\n",
    "- Report the accuracy of this ensemble model.\n",
    "- Is the ensemble's accuracy higher than the accuracy of each of the three individual models run separately? Why might this be the case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96396211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 3 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 3 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca11903",
   "metadata": {},
   "source": [
    "## Question 4: Soft Voting Ensemble: Change the voting parameter in your VotingClassifier to 'soft'. This requires all estimators to have a predict_proba method. How does the accuracy of soft voting compare to hard voting? Explain the mechanical difference between these two voting strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813da9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 4 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 4 END ############\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
