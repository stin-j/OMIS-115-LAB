{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff54f882",
   "metadata": {},
   "source": [
    "# General Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a884d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3927a",
   "metadata": {},
   "source": [
    "# Additional Import Code for dataset C (PART 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c4f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "california_housing.frame.head()\n",
    "X_1 = california_housing.data \n",
    "y_1 = california_housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa1321",
   "metadata": {},
   "source": [
    "# Additional Import Code for dataset W (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fd46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "X_2 = wine_quality.data.features \n",
    "y_2 = wine_quality.data.targets['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b45d4",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313eed1",
   "metadata": {},
   "source": [
    "## Question 1: Baseline Model: Train a RandomForestRegressor with n_estimators=100. What is the R-squared (R2) score on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f52c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Assignment 5 PART 1 Question 1 BEGIN ############\n",
      "RandomForestRegressor with n_estimators=100\n",
      "R-squared (test set): 0.7942\n",
      "############ Assignment 5 PART 1 Question 1 END ############\n"
     ]
    }
   ],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 1 BEGIN ############\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "# Train a RandomForestRegressor with n_estimators=100\n",
    "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split (X_1, y_1, test_size=0.25, random_state=0)\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=0) \n",
    "rf_regressor.fit(X_1_train, y_1_train)\n",
    "\n",
    "# What is the R-squared on test set?\n",
    "y_1_pred = rf_regressor.predict(X_1_test)\n",
    "r2_q1 = r2_score(y_1_test, y_1_pred)\n",
    "\n",
    "print(\"RandomForestRegressor with n_estimators=100\")\n",
    "print(f\"R-squared (test set): {r2_q1:.4f}\")\n",
    "print(\"############ Assignment 5 PART 1 Question 1 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db9446",
   "metadata": {},
   "source": [
    "## Question 2: Number of Trees: The n_estimators parameter defines the number of trees in the forest. Train two additional models: one with n_estimators=5 and another with n_estimators=500. How does the R2 score change as the number of trees increases? Why does a random forest generally not overfit by simply adding more trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4f406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Assignment 5 PART 1 Question 2 BEGIN ############\n",
      "RandomForestRegressor with n_estimators=5\n",
      "R-squared (test set): 0.7578\n",
      "\n",
      "RandomForestRegressor with n_estimators=500\n",
      "R-squared (test set): 0.7955\n",
      "We see that as the number of trees increases, the R2 score increases (0.7578 -> 0.7955 when going from n_estimators=5 -> 500) \n",
      "\n",
      "Random forests generally don't overfit from adding more trees because each tree only looks at part of the data whether it be through looking at some of the observations (bootstrapping) or some of the features (max_features). So, each individual tree doesn't just memorize the entire training dataset. They each learn a different part of it, allowing their outputs to better generalize\n",
      "############ Assignment 5 PART 1 Question 2 END ############\n"
     ]
    }
   ],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 2 BEGIN ############\")\n",
    "# Train two additional models: one with n_estimators=5 and another with n_estimators=500\n",
    "rf_regressor_5 = RandomForestRegressor(n_estimators=5, random_state=0) \n",
    "rf_regressor_5.fit(X_1_train, y_1_train)\n",
    "\n",
    "rf_regressor_500 = RandomForestRegressor(n_estimators=500, random_state=0) \n",
    "rf_regressor_500.fit(X_1_train, y_1_train)\n",
    "\n",
    "# How does R2 score change as number of trees increases\n",
    "y_1_pred_5 = rf_regressor_5.predict(X_1_test)\n",
    "r2_q2_5 = r2_score(y_1_test, y_1_pred_5)\n",
    "print(\"RandomForestRegressor with n_estimators=5\")\n",
    "print(f\"R-squared (test set): {r2_q2_5:.4f}\")\n",
    "\n",
    "y_1_pred_500 = rf_regressor_500.predict(X_1_test)\n",
    "r2_q2_500 = r2_score(y_1_test, y_1_pred_500)\n",
    "print(\"\\nRandomForestRegressor with n_estimators=500\")\n",
    "print(f\"R-squared (test set): {r2_q2_500:.4f}\")\n",
    "\n",
    "print(\"We see that as the number of trees increases, the R2 score increases (0.7578 -> 0.7955 when going from n_estimators=5 -> 500) \")\n",
    "\n",
    "# Why does random forest not overfit (WIP; double check this explanation)\n",
    "print(\"\\nRandom forests generally don't overfit from adding more trees because each tree only looks at part of the data whether it be through looking at some of the observations (bootstrapping) or some of the features (max_features). So, each individual tree doesn't just memorize the entire training dataset. They each learn a different part of it, allowing their outputs to better generalize\")\n",
    "print(\"############ Assignment 5 PART 1 Question 2 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f123e2c",
   "metadata": {},
   "source": [
    "## Question 3: Feature Sub-sampling: The power of random forests comes from decorrelating the trees. The max_features parameter controls this. Train a model with max_features=0.5 (using 50% of features for each tree) and compare its performance to a model with max_features=None (which is equivalent to a Bagging Regressor). Which performs better, and why does this feature sub-sampling often lead to a more robust model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Assignment 5 PART 1 Question 3 BEGIN ############\n",
      "RandomForestRegressor with max_features=0.5\n",
      "R-squared (test set): 0.8073\n",
      "\n",
      "RandomForestRegressor with max_features=None\n",
      "R-squared (test set): 0.7942\n",
      "Based on R-squared, the model that performed better was the RandomForestRegressor where max_features=0.5.\n",
      "\n",
      "Feature sub-sampling often leads to a more robust model because...\n",
      "############ Assignment 5 PART 1 Question 3 END ############\n"
     ]
    }
   ],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 3 BEGIN ############\")\n",
    "# Train a model with max_features=0.5 and a model with max_features=None\n",
    "rf_regressor_50f = RandomForestRegressor(max_features=0.5, random_state=0) \n",
    "rf_regressor_50f.fit(X_1_train, y_1_train)\n",
    "y_1_pred_50f = rf_regressor_50f.predict(X_1_test)\n",
    "r2_q3_50f = r2_score(y_1_test, y_1_pred_50f)\n",
    "\n",
    "rf_regressor_nonef = RandomForestRegressor(max_features=None, random_state=0) \n",
    "rf_regressor_nonef.fit(X_1_train, y_1_train)\n",
    "y_1_pred_nonef = rf_regressor_nonef.predict(X_1_test)\n",
    "r2_q3_nonef = r2_score(y_1_test, y_1_pred_nonef)\n",
    "        # essentially the same model as from question 1\n",
    "\n",
    "# Compare their performances, which performs better\n",
    "print(\"RandomForestRegressor with max_features=0.5\")\n",
    "print(f\"R-squared (test set): {r2_q3_50f:.4f}\")\n",
    "print(\"\\nRandomForestRegressor with max_features=None\")\n",
    "print(f\"R-squared (test set): {r2_q3_nonef:.4f}\")\n",
    "\n",
    "print(\"Based on R-squared, the model that performed better was the RandomForestRegressor where max_features=0.5.\")\n",
    "\n",
    "# Why does feature sub-sampling lead to more robust model (WIP)\n",
    "print(\"\\nFeature sub-sampling often leads to a more robust model because it reduces correlation among individual trees in the forest. \" \\\n",
    "\"When trees are less correlated, their errors cancel each other out when aggregating predictions, leading to improved generalization performance. \" \\\n",
    "\"Also by considering different subsets of features at each split, the model can capture diverse patterns in the data which helps in reducing overfitting\")\n",
    "print(\"############ Assignment 5 PART 1 Question 3 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cc48c",
   "metadata": {},
   "source": [
    "## Question 4: Comparison to a Single Tree: Train a single DecisionTreeRegressor with no depth constraints on the same data. How does its R2 score compare to your best RandomForestRegressor? Explain conceptually why an ensemble of trees (Random Forest) typically outperforms a single, complex tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fabbf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Assignment 5 PART 1 Question 4 BEGIN ############\n",
      "Single DecisionTreeRegressor\n",
      "R-squared (test set): 0.5838\n",
      "\n",
      "Best RandomForestRegressor\n",
      "R-squared (test set): 0.8073\n",
      "The ensemble outperformed the single tree because a single tree will grow too complex and overfit to the data. By creating multiple different trees from the same dataset, we can reduce overfitting and reduce variance, improving our model's performance.\n",
      "############ Assignment 5 PART 1 Question 4 END ############\n"
     ]
    }
   ],
   "source": [
    "print(\"############ Assignment 5 PART 1 Question 4 BEGIN ############\")\n",
    "# Train a single DecisionTreeRegressor with no depth constraints on the same data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt_regressor = DecisionTreeRegressor(random_state=0)\n",
    "dt_regressor.fit(X_1_train, y_1_train)\n",
    "\n",
    "# How does its R2 score compare to your best RandomForestRegressor?\n",
    "y_1_pred_dt = dt_regressor.predict(X_1_test)\n",
    "r2_q4 = r2_score(y_1_test, y_1_pred_dt)\n",
    "print(\"Single DecisionTreeRegressor\")\n",
    "print(f\"R-squared (test set): {r2_q4:.4f}\")\n",
    "\n",
    "best_r2 = max(r2_q1, r2_q2_5, r2_q2_500, r2_q3_50f, r2_q3_nonef)\n",
    "print(\"\\nBest RandomForestRegressor\")\n",
    "print(f\"R-squared (test set): {best_r2:.4f}\")\n",
    "\n",
    "# Why does ensemble of trees outperform single tree (WIP; double-check this explanation)\n",
    "print(\"The ensemble outperformed the single tree because a single tree will grow too complex and overfit to the data. By creating multiple different trees from the same dataset, we can reduce overfitting and reduce variance, improving our model's performance.\")\n",
    "print(\"############ Assignment 5 PART 1 Question 4 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c86e31",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b97fb",
   "metadata": {},
   "source": [
    "## Question 1: Boosting: Train a GradientBoostingClassifier with n_estimators=100 on the combined wine dataset. Report its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7c4b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Assignment 5 PART 2 Question 1 BEGIN ############\n",
      "accuracy: 0.8441\n",
      "############ Assignment 5 PART 2 Question 1 END ############\n"
     ]
    }
   ],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 1 BEGIN ############\")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets['quality']\n",
    "y_binary = (y >= 7).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42, stratify=y_binary)\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "print(\"############ Assignment 5 PART 2 Question 1 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582863c1",
   "metadata": {},
   "source": [
    "## Question 2: Boosting vs. Bagging: How does the accuracy of the GradientBoostingClassifier compare to a RandomForestClassifier (a bagging method) with the same n_estimators? Explain the fundamental difference in how boosting and bagging build their sequential vs. parallel ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37005771",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 2 BEGIN ############\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 2 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2765ee",
   "metadata": {},
   "source": [
    "## Question 3: Hard Voting Ensemble (Stacking): Create a VotingClassifier that combines three different base models: a LogisticRegression(max_iter=2000), a DecisionTreeClassifier(max_depth=5), and a KNeighborsClassifier(n_neighbors=7). Use the default voting='hard'.\n",
    "- Report the accuracy of this ensemble model.\n",
    "- Is the ensemble's accuracy higher than the accuracy of each of the three individual models run separately? Why might this be the case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96396211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 3 BEGIN ############\")\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 3 END ############\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca11903",
   "metadata": {},
   "source": [
    "## Question 4: Soft Voting Ensemble: Change the voting parameter in your VotingClassifier to 'soft'. This requires all estimators to have a predict_proba method. How does the accuracy of soft voting compare to hard voting? Explain the mechanical difference between these two voting strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813da9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"############ Assignment 5 PART 2 Question 4 BEGIN ############\")\n",
    "\n",
    "print(\"############ Assignment 5 PART 2 Question 4 END ############\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
